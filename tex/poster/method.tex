\section{Method}
\label{sec:method}


* Collection of data:
	* MNIST data
* Methods (described to a level that others can reproduce the results):
	* Diagram (denote stochastic layers)
	* Downsampling (average pool layer)
	* VAE
		* Auto-encoder
		* Variational bound
	* Fully-connected neural network (NN)
	* Convolutional neural network (CNN) (*maybe*)
* Experiments:
	* Reconstruction varying
		* downsampling factor
		* number of latent size
	* Super-resolution of home-made numbers


\subsection{Data} % (fold)
\label{sub:data}
\begin{itemize}
	\item MNIST dataset \cite{MNIST}, $\vec{X} = \curlies{\vec{X}\order{i}}^N_{i=1}$, of i.i.d. images of handwritten digits. 
	\item From downsampled low resolution images (LR) $\vec{X}_{LR}$ to high resolution images (HR) $\vec{X}_{LR}$. 
\end{itemize}
% subsection data (end)

\subsection{The Model: Variational Auto-Encoder} % (fold)
\label{sub:the_model}
%TODO: Tilføj diagram

\begin{itemize}
	\item \textbf{Goal}: Infer same structure from LR to HR. 
	\item \textbf{Solution}: Model assume the same underlying probabilistic data generating process behind the production of LR and HR digits.
	\item \textbf{Variational Auto-Encoder}: $\vec{X}_HR$ $\rightarrow$ Downsample, $\vec{X}_LR$ $\rightarrow$  Encoder, $\enc{\vec{z}|\vec{x}}$ $\rightarrow$ latent space, $\vec{z}$ $\rightarrow$ Decoder, $\dec{\vec{x}|\vec{z}}$ $\rightarrow$ Reconstruction, $\tilde{\vec{X}}_HR$
	\item \textbf{Loss-function}: Maximize variational lower bound, $\mathcal{L}\parens{\vec{\theta},\vec{\phi};\vec{x}}$ derived from the mean-field variational Bayes where the marginal log-likelihood can be decomposed like for the EM-algorithm and variational inference \cite[\S10.2]{Bishop2006}:
	\begin{equation}
		\log \dec{\vec{X}\order{i}} = D_{KL}\left( \enc{\vec{z}|\vec{x}}||\dec{\vec{z}|\vec{x}}\right) + \mathcal{L}\left(\vec{\theta},\vec{\phi};\vec{x}\right)
	\end{equation} 
	where the non-negativity of the Kulback-Leibler divergence makes the Free energy $\mathcal{L}\parens{\vec{\theta},\vec{\phi};\vec{x}}$ a lower bound for the marginal log-likelihood:
	\begin{gather}
	\begin{split}
		\log \dec{\vec{x}} & \ge \mathcal{L}\left(\vec{\theta},\vec{\phi};\vec{x}\right)
		\\ & =
		\int \enc{\vec{z}|\vec{x}} \log \curlies*{ \frac{\dec{\vec{x},\vec{z}}}{\enc{\vec{z}|\vec{x}} } } \D{\vec{z}} \\ 
		& = \E_{\enc{\vec{z}|\vec{x}}} \brackets{- \log \enc{\vec{z}|\vec{x}} + \log \dec{\vec{x},\vec{z}} } 
		\\
		& = -D_{KL}\parens{ \enc{\vec{z}|\vec{x}}||\dec{\vec{z}} } 
		\\ 
		& \quad+ \E_{\enc{\vec{z}|\vec{x}}} \brackets{\log \dec{\vec{x}|\vec{z}} }   	
	\end{split} 
\end{gather}
\end{itemize}

% subsection the_model (end)
For the reconstruction of high-resolution (HR) digits from low-resolution (LR) digits in the MNIST dataset, our model assume the same underlying data generating process behind the production of digits to infer the same structure between them. Each variable in $\vec{X}$ are assumed to be drawn from a normal distribution.\dots 
%TODO: Skriv mere beskrivelse af modellen og indsæt model diagram
Using mean-field variational Bayes \cite{Kingma2014}, we build a generative model with an approximated decoding posterior distribution, $\enc{\vec{z}|\vec{X}}$ mapping the observed variables $\vec{X}$ into the unobserved latent variables $\vec{z}$. The generative part of the model is then using the conditional likelihood $\dec{\vec{X},\vec{z}}$ to  the process  .  based on a per-pixel loss-function  marginal log-likelihood of each datapoint, $i$:
\begin{equation}
	\log \dec{\vec{X}} = \log \dec{\vec{X}\order{1}, ..., \vec{X}\order{N}} = \sum^N_{i=1} \log \enc{\vec{X}\order{i}} 
\end{equation}
From now on we denote $\vec{X}\order{i} = \vec{x}$ for shorter and uncluttered notation.
The marginal log-likelihood can be decomposed like for the EM-algorithm and variational inference \cite[\S10.2]{Bishop2006}:
\begin{equation}
	\log \dec{\vec{X}\order{i}} = D_{KL}\left( \enc{\vec{z}|\vec{x}}||\dec{\vec{z}|\vec{x}}\right) + \mathcal{L}\left(\vec{\theta},\vec{\phi};\vec{x}\right)
\end{equation} 
Here the first term is the Kullback-Leibler divergence, which is a non-negative entropy measure of how much the 2 distributions differ.
\begin{equation}
	D_{KL}\left( \enc{\vec{z}|\vec{x}}||\dec{\vec{z}|\vec{x}}\right) = \int \enc{\vec{z}|\vec{x}} \log \curlies*{ \frac{\dec{\vec{x}|\vec{z}}}{\enc{\vec{z}|\vec{x}} } } \D{\vec{z}}
\end{equation}

Since the KL-divergence is non-negative the second term (``free energy'') works as a variational lower bound and follows the inequality:

\begin{gather}
	\begin{split}
		\log \dec{\vec{x}} & \ge \mathcal{L}\left(\vec{\theta},\vec{\phi};\vec{x}\right)
		\\ & =
		\int \enc{\vec{z}|\vec{x}} \log \curlies*{ \frac{\dec{\vec{x},\vec{z}}}{\enc{\vec{z}|\vec{x}} } } \D{\vec{z}} \\ 
		& = \E_{\enc{\vec{z}|\vec{x}}} \brackets{- \log \enc{\vec{z}|\vec{x}} + \log \dec{\vec{x},\vec{z}} } 
		\\
		& = -D_{KL}\parens{ \enc{\vec{z}|\vec{x}}||\dec{\vec{z}} } 
		\\ 
		& \quad+ \E_{\enc{\vec{z}|\vec{x}}} \brackets{\log \dec{\vec{x}|\vec{z}} }   	
	\end{split} 
\end{gather}

Maximizing $\mathcal{L}$ wrt. the model parameters, $\phi$ and $\theta$, thereby minimizes the $D_{KL}\left( \enc{\vec{z}|\vec{x}}||\dec{\vec{z}|\vec{x}}\right)$ bringing the . When $D_{KL}\rightarrow 0$ the approximated variational distribution $\enc{\vec{z}|\vec{x}}\rightarrow \dec{\vec{z}|\vec{x}}$

Differentiation of L
Reparameretization trick
Output parameters (mu and sigma)