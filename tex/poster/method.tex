%!TEX root = protokoll.tex
\section{Method}
\label{sec:method}

Based on Kingma's paper on \texttt{Auto-Encoding Variational Bayes} \cite{Kingma2014}, we build a similar model using a per-pixel loss-function based on the marginal log-likelihood of each datapoint, $i$:
\begin{equation}
	\log \dec{\vec{x}\order{1}, ..., \vec{x}\order{N}} = \sum^N_{i=1} \log \enc{\vec{x}\order{i}} 
\end{equation}
where this can be decomposed like for the EM-algorithm and in the same way as Bishop performs variational inference for eq. 10.2 on page 463 in \cite{Bishop2006}:
\begin{equation}
	\log \dec{\vec{x}\order{i}} = D_{KL}\left( \enc{\vec{z}|\vec{x}\order{i}}||\dec{\vec{z}|\vec{x}\order{i}}\right) + \mathcal{L}\left(\vec{\theta},\vec{\phi};\vec{x}\order{i}\right)
\end{equation} 
Here the first term is the Kullback-Leibler divergence, which is a non-negative entropy measure of how much the 2 distributions differ.
\begin{equation}
	D_{KL} = \int \enc{\vec{z}|\vec{x}} \log \curlies*{ \frac{\dec{\vec{x}|\vec{z}}}{\enc{\vec{z}|\vec{x}} } } \D{\vec{z}}
\end{equation}

Since the KL-divergence is non-negative the second term (``free energy'') works as a variational lower bound and follows the inequality:
\begin{align}
	\log \dec{\vec{x}\order{i}} \ge \mathcal{L}\left(\vec{\theta},\vec{\phi};\vec{x}\order{i}\right) = \int \enc{\vec{z}|\vec{x}\order{i}} \log \curlies*{ \frac{\dec{\vec{x},\vec{z}}}{\enc{\vec{z}|\vec{x}\order{i}} } } \D{\vec{z}} = \E_{\enc{\vec{z}|\vec{x}}} [- \log \enc{\vec{z}|\vec{x}}] 	
\end{align} 
Maximizing $\mathcal{L}$ wrt. the model parameters, $\phi$ and $\theta$, thereby minimizes the $D_{KL}$. When $D_{KL}\rightarrow 0$ the approximated variational distribution $\enc{\vec{z}|\vec{x}}\rightarrow \dec{\vec{z}|\vec{x}}$

Differentiation of L
Reparameretization trick
Output parameters (mu and sigma)
