\chapter{Theory}
\label{cha:theory}

Based on Kingma's paper on \texttt{Auto-Encoding Variational Bayes} \cite{Kingma2014}, we build a similar model using a per-pixel loss-function based on the marginal log-likelihood of each datapoint, $i$:
\begin{equation}
	\log \dec{\vec{x}^{(1)}, ..., \vec{x}^{(N)}} = \sum^N_{i=1} \log \enc{\vec{x}^{(i)}} 
\end{equation}
where this can be decomposed like for the EM-algorithm and in the same way as Bishop performs variational inference for eq. 10.2 on page 463 in \cite{Bishop2006}:
\begin{equation}
	\log \dec{\vec{x}^{(i)}} = D_{KL}\left( \enc{\vec{z}|\vec{x}^{(i)}}||\dec{\vec{z}|\vec{x}^{(i)}}\right) + \mathcal{L}\left(\vec{\theta},\vec{\phi};\vec{x}^{(i)}\right)
\end{equation} 
Here the first term is the Kullback-Leibler divergence, which is a non-negative entropy measure of how much the 2 distributions differ.
\begin{equation}
	D_{KL} = \int \enc{\vec{z}|\vec{x}} \log \curlies*{ \frac{\dec{\vec{x}|\vec{z}}}{\enc{\vec{z}|\vec{x}} } } \D{\vec{z}}
\end{equation}

Since the KL-divergence is non-negative the second term (``free energy'') works as a variational lower bound. 
\begin{align}
	\mathcal{L}\left(\vec{\theta},\vec{\phi};\vec{x}^{(i)}\right) = \int \enc{\vec{z}|\vec{x}^{(i)}} \log \curlies*{ \frac{\dec{\vec{x},\vec{z}}}{\enc{\vec{z}|\vec{x}^{(i)}} } } \D{\vec{z}} 	
\end{align} 
Maximizing $\mathcal{L}$ wrt. the model parameters, $\phi$ and $\theta$, thereby minimizes the $D_{KL}$. When $D_{KL}\rightarrow 0$ the approximated variational distribution $\enc{\vec{z}|\vec{x}}\rightarrow \dec{\vec{z}|\vec{x}}$

Differentiation of L
Reparameretization trick
Output parameters (mu and sigma)
